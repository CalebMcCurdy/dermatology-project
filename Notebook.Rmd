---
title: "ADS 503: Final Team Project"
author: "Caleb McCurdy, Mirna Philip, and Christine Vu"
date: "June 26, 2023"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***

## Data Importing and Pre-processing
```{r message = FALSE, warning = FALSE, results = 'hide'}
library(readr)
library(readxl)
library(ggplot2)
library(tidyr)
library(dplyr)
library(corrplot)
library(mlbench)
library(moments)
library(e1071)
library(caret)
library(Hmisc)
library(plotly)
library(psych)
library(MASS)
library(AppliedPredictiveModeling)
library(glmnet)
```

```{r}
df <- read.table('https://raw.githubusercontent.com/mirna1983/dermatology-project/main/data/dermatology.data', sep = ",")

head(df, 6)
```

```{r}
# Dimension of data (rows and columns)
dim(df)
```

&nbsp;

#### Add Column Names

The data frame is missing the column names that correspond to the provided attribute list. Therefore, the column names are appended based on the attribute information list.

```{r, warnings = FALSE}
# Desired column names
desired_columns <- c("erythema", "scaling", "definite_borders", "itching", "koebner_phenomenon", "polygonal_papules", "follicular_papules", "oral_mucosal_involvement", "knee_and_elbow_involvement", "scalp_involvement", "family_history", "melanin_incontinence", "eosinophils_in_the_infiltrate", "PNL_infiltrate", "fibrosis_of_the_papillary_dermis", "exocytosis", "acanthosis", "hyperkeratosis", "parakeratosis", "clubbing_of_the_rete_ridges", "elongation_of_the_rete_ridges", "thinning_of_the_suprapapillary_epidermis", "spongiform_pustule", "munro_microabcess", "focal_hypergranulosis", "disappearance_of_the_granular_layer", "vacuolisation_and_damage_of_basal_layer", "spongiosis", "saw-tooth_appearance_of_retes", "follicular_horn_plug", "perifollicular_parakeratosis", "inflammatory_monoluclear_inflitrate", "band-like_infiltrate", "Age", "class")

# Assign desired column names
colnames(df) <- desired_columns
head(df, 3)
```

```{r}
# Summary of the data frame
summary(df)
```

```{r}
str(df)
```

```{r}
# Change 'Age' into type numeric
df$Age <- as.numeric(df$Age)
```


```{r}
# Check for missing values
sapply(df, function(x) sum(is.na(x)))
```

There are no missing values in this data set other than for the 'Age' predictor which has 8. Due to the small number of instances with missing values, we will delete these rows from the data set.

```{r}
rows_with_NA <- which(is.na(df$Age))
rows_with_NA

df <- df[-rows_with_NA, ]
df
```

```{r}
# Check for potential duplicate instances
sum(duplicated(as.matrix(df), fromLast = FALSE))
```

There are no duplicate instances to deal with.

```{r}
# Make the graphs smaller by having 3 rows of graphs and 4 columns 
par(mfrow = c(3,4))

hist.data.frame(df)
```

```{r}
# Plotting the correlation matrix using the corrplot function, only using 
# complete pairs of observations between variables
corrplot(cor(df), cl.cex= .5,tl.cex=.4)
```

```{r}
# plotting boxplots of the 34 predictors and response variable
boxplot(df[1:7])
boxplot(df[8:14])
boxplot(df[15:21])
boxplot(df[22:28])
boxplot(df[29:35])
```

```{r}
# Skewness of numeric predictors
skewness_values <- apply(df, 2, skewness, na.rm = TRUE)
skewness_values
```

```{r}
pred <- df[,-35]
class <- df$class

# Split the data into training (80%) and test (20%) sets
set.seed(123)
training <- createDataPartition(class, p = 0.8, list = FALSE)
pred_train <- pred[training,]
class_train <- class[training]
pred_test <- pred[-training,]
class_test <- class[-training]

# pre-process the data
preProcValues <- preProcess(pred_train, method = c("center", "scale"))

pred_train_trans <- predict(preProcValues, pred_train)
pred_test_trans <- predict(preProcValues, pred_test)

indx <- createFolds(class_train, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = indx, number = 10)
```

```{r}
## USE pred_train_trans AS THE TRAIN X SET
## USE pred_test_trans AS THE TEST X SET
## USE class_train AS THE TRAIN Y SET
## USE class_test AS THE TEST Y SET
```


#### Neural network
```{r}
# Defining the grid of hyperparameters for the neural network
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(3, 7, 11, 13))
# Setting seed for reproducibility
set.seed(123)
# Training the neural network
nnetTune <- train(x = pred_train_trans, y = class_train,
                  method = "nnet",
                  tuneGrid = nnetGrid,
                  trControl = ctrl,
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 13 * (ncol(pred_train_trans) + 1) + 13 + 1,
                  maxit = 1000)
nnetTune
```
```{r}
#finding the row with the lowest RMSE
nnetTune$results[nnetTune$results$RMSE == min(nnetTune$results$RMSE), 3:5]

testResults <- data.frame(obs = class_test,
                          NNet = predict(nnetTune, pred_test_trans))
```

#### Multivariate adaptive regression splines (MARS)
```{r}
set.seed(123)
marsTune <- train(x = pred_train_trans, y = class_train,
                  method = "earth",
                  tuneGrid = expand.grid(degree = 1, nprune = 2:38),
                  trControl = ctrl)
marsTune

plot(marsTune)

#finding the row with the lowest RMSE
marsTune$results[marsTune$results$RMSE == min(marsTune$results$RMSE), 3:5]

testResults$MARS <- predict(marsTune, pred_test_trans)
```

#### k-nearest neighbors
```{r}
knnDescr <- pred_train_trans[, -nearZeroVar(pred_train_trans)]

set.seed(123)
knnTune <- train(x = knnDescr, y = class_train,
                 method = "knn",
                 tuneGrid = data.frame(k = 1:20),
                 trControl = ctrl)
                 
knnTune

plot(knnTune)

#finding the row with the lowest RMSE
knnTune$results[knnTune$results$RMSE == min(knnTune$results$RMSE), 2:4]

testResults$Knn <- predict(knnTune, pred_test_trans[, names(knnDescr)])
```

#### Support vector machine (Linear)
```{r, warning = FALSE}
set.seed(123)

svm_lin <- train(x = pred_train_trans, y = class_train,
                  method = "svmLinear", tuneLength = 10,
                  trControl = ctrl)
```

#### Support vector machine (RBF)
```{r, warning = FALSE}
set.seed(123)

svm_rad <- train(x = pred_train_trans, y = class_train,
                  method = "svmRadial", tuneLength = 10,
                  trControl = ctrl)
```

#### Support vector machine (Polynomial kernel)
```{r, warning = FALSE}
set.seed(123)

svm_poly <- train(x = pred_train_trans, y = class_train,
                  method = "svmPoly", tuneLength = 10,
                  trControl = ctrl)
```

```{r}
# Variable importance
nnetImp <- varImp(nnetTune, scale = FALSE)
plot(nnetImp, top = 20)
marsImp <- varImp(marsTune, scale = FALSE)
plot(marsImp)
knnImp <- varImp(knnTune, scale = FALSE)
plot(knnImp, top = 20)
svmLImp <- varImp(svm_lin, scale = FALSE)
plot(svmLImp, top = 20)
svmRImp <- varImp(svm_rad, scale = FALSE)
plot(svmRImp, top = 20)
svmPImp <- varImp(svm_poly, scale = FALSE)
plot(svmPImp, top = 20)
```